{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "Ejemplo de clasificador con Pytorch, comentado en espa침ol. \n",
    "\n",
    "El cuaderno de Jupyter se ha desarrollado con Visual Code y puedes encontrar el c칩digo en https://github.com/josegemez/pytorch_mnist \n",
    "\n",
    "Se ha desarrollado en python 3.10.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "Con las primeras lineas vamos a importar los paquetes que b치sicos de Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 4\n",
    "batch_size_test = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms=torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #media y desviacion estandar para el dataset de cifar en concreto\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transforms)  #descarga el dataset de pytorch                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_cifar_train,  batch_size=batch_size_train, shuffle=True,num_workers=0,pin_memory=True) #cargador de los datos en batch de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que \"pinta\" tiene cada elemento del cargador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader)) #asignamos el primer batch a las variables x e y. La variable X contrendra las imagenes e y contrendra las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x es un batch, por lo que la primera dimensi칩n coincidira con el batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = x[0].numpy()\n",
    "img0 = img0/ 2 + 0.5 #para quitar la normalizacion que hemos hecho antes con los transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(np.transpose(img0, (1, 2, 0))); # transformamos el primer elemento del batch una matriz de numpy y mostramos con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5373,  0.5216,  0.5137,  ...,  0.5059,  0.3569, -0.0196],\n",
       "          [ 0.6784,  0.7098,  0.7020,  ..., -0.0667, -0.1216, -0.0510],\n",
       "          [ 0.5529,  0.5451,  0.4980,  ..., -0.2157, -0.2471, -0.1608],\n",
       "          ...,\n",
       "          [ 0.4510,  0.6000,  0.6863,  ..., -0.2627, -0.2471, -0.2627],\n",
       "          [ 0.8745,  0.8353,  0.7961,  ..., -0.2863, -0.3020, -0.3020],\n",
       "          [ 0.8980,  0.8588,  0.8745,  ..., -0.3569, -0.3098, -0.3490]],\n",
       "\n",
       "         [[ 0.3490,  0.3333,  0.3333,  ...,  0.4510,  0.3020, -0.0745],\n",
       "          [ 0.4902,  0.5451,  0.5059,  ..., -0.1059, -0.1451, -0.0902],\n",
       "          [ 0.4039,  0.3804,  0.3804,  ..., -0.2471, -0.2706, -0.1922],\n",
       "          ...,\n",
       "          [ 0.4980,  0.6235,  0.7020,  ..., -0.2627, -0.2471, -0.2627],\n",
       "          [ 0.9059,  0.8510,  0.8118,  ..., -0.2863, -0.2941, -0.2863],\n",
       "          [ 0.9294,  0.8980,  0.9137,  ..., -0.3569, -0.3020, -0.3412]],\n",
       "\n",
       "         [[ 0.2157,  0.2157,  0.2314,  ...,  0.3412,  0.1765, -0.1765],\n",
       "          [ 0.3490,  0.3882,  0.3569,  ..., -0.1843, -0.2235, -0.1843],\n",
       "          [ 0.2863,  0.2706,  0.2627,  ..., -0.3255, -0.3255, -0.2549],\n",
       "          ...,\n",
       "          [ 0.5529,  0.6784,  0.7490,  ..., -0.2627, -0.2471, -0.2627],\n",
       "          [ 0.9294,  0.8745,  0.8510,  ..., -0.2941, -0.3098, -0.2863],\n",
       "          [ 0.9686,  0.9137,  0.9373,  ..., -0.3647, -0.3176, -0.3412]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4431,  0.3804,  0.3647,  ...,  0.0196, -0.0039, -0.0196],\n",
       "          [ 0.3176,  0.2863,  0.2784,  ..., -0.0039, -0.0039,  0.0196],\n",
       "          [ 0.2000,  0.2000,  0.2000,  ..., -0.0902, -0.0353,  0.0275],\n",
       "          ...,\n",
       "          [-0.3333, -0.3333, -0.3255,  ..., -0.6157, -0.6549, -0.6471],\n",
       "          [-0.3412, -0.3255, -0.3490,  ..., -0.7020, -0.7255, -0.7020],\n",
       "          [-0.3882, -0.4039, -0.4039,  ..., -0.7098, -0.7020, -0.7020]],\n",
       "\n",
       "         [[ 0.6627,  0.6235,  0.6157,  ...,  0.2000,  0.1686,  0.1529],\n",
       "          [ 0.5294,  0.5137,  0.5059,  ...,  0.1529,  0.1529,  0.1608],\n",
       "          [ 0.3882,  0.3961,  0.3961,  ...,  0.0353,  0.0902,  0.1373],\n",
       "          ...,\n",
       "          [-0.5765, -0.5686, -0.5608,  ..., -0.8196, -0.8275, -0.8196],\n",
       "          [-0.6000, -0.5686, -0.5843,  ..., -0.8824, -0.8980, -0.8745],\n",
       "          [-0.6471, -0.6392, -0.6392,  ..., -0.8902, -0.8824, -0.8824]],\n",
       "\n",
       "         [[ 0.8510,  0.8039,  0.8039,  ...,  0.5294,  0.5059,  0.5373],\n",
       "          [ 0.7569,  0.7333,  0.7255,  ...,  0.4824,  0.4824,  0.5216],\n",
       "          [ 0.6471,  0.6314,  0.6314,  ...,  0.3647,  0.4118,  0.4667],\n",
       "          ...,\n",
       "          [-0.4980, -0.4980, -0.5059,  ..., -0.7020, -0.7255, -0.7255],\n",
       "          [-0.5451, -0.5216, -0.5451,  ..., -0.7725, -0.7725, -0.7882],\n",
       "          [-0.5922, -0.6000, -0.6000,  ..., -0.7725, -0.7490, -0.7961]]],\n",
       "\n",
       "\n",
       "        [[[-0.2078, -0.1294, -0.0667,  ..., -0.0745, -0.1373, -0.2549],\n",
       "          [-0.2000, -0.1765, -0.0510,  ..., -0.1451, -0.2549, -0.4118],\n",
       "          [ 0.0039, -0.0431,  0.0196,  ..., -0.1137, -0.2471, -0.1843],\n",
       "          ...,\n",
       "          [ 0.0745,  0.0353,  0.3490,  ...,  0.2078,  0.4745,  0.5216],\n",
       "          [ 0.2235,  0.2863,  0.1451,  ...,  0.1373,  0.1922, -0.0745],\n",
       "          [ 0.6078,  0.7098,  0.5765,  ...,  0.0902, -0.0824, -0.3804]],\n",
       "\n",
       "         [[-0.1451, -0.0745, -0.0275,  ..., -0.1373, -0.1608, -0.2078],\n",
       "          [-0.1608, -0.1373, -0.0196,  ..., -0.1608, -0.2392, -0.3412],\n",
       "          [ 0.0353, -0.0275,  0.0196,  ..., -0.1294, -0.2471, -0.1608],\n",
       "          ...,\n",
       "          [-0.0431, -0.0353,  0.3020,  ...,  0.0039,  0.2706,  0.3255],\n",
       "          [ 0.1216,  0.2000,  0.0745,  ..., -0.1137, -0.0431, -0.2863],\n",
       "          [ 0.5216,  0.6314,  0.4824,  ..., -0.1686, -0.3176, -0.5529]],\n",
       "\n",
       "         [[-0.3725, -0.4196, -0.4745,  ..., -0.3725, -0.4353, -0.5294],\n",
       "          [-0.3882, -0.5294, -0.4980,  ..., -0.4353, -0.5451, -0.6941],\n",
       "          [-0.2392, -0.4588, -0.4431,  ..., -0.3333, -0.4667, -0.4196],\n",
       "          ...,\n",
       "          [-0.1451, -0.1843,  0.1216,  ..., -0.2471,  0.1373,  0.2863],\n",
       "          [ 0.0275,  0.0745, -0.0824,  ..., -0.3804, -0.1843, -0.3412],\n",
       "          [ 0.4431,  0.5294,  0.3647,  ..., -0.3882, -0.4510, -0.6706]]],\n",
       "\n",
       "\n",
       "        [[[-0.6627, -0.6706, -0.6784,  ..., -0.2157, -0.5686, -0.2863],\n",
       "          [-0.7490, -0.7333, -0.7412,  ..., -0.4431, -0.6000, -0.5922],\n",
       "          [-0.7412, -0.7412, -0.7412,  ..., -0.5922, -0.5922, -0.5765],\n",
       "          ...,\n",
       "          [-0.6706, -0.6784, -0.7333,  ..., -0.5765, -0.5059, -0.5137],\n",
       "          [-0.7176, -0.6706, -0.7176,  ..., -0.5216, -0.4980, -0.5137],\n",
       "          [-0.7412, -0.6784, -0.7020,  ..., -0.4745, -0.4588, -0.4902]],\n",
       "\n",
       "         [[-0.6784, -0.6863, -0.6863,  ..., -0.2863, -0.6157, -0.2941],\n",
       "          [-0.7647, -0.7490, -0.7569,  ..., -0.5765, -0.6941, -0.6235],\n",
       "          [-0.7569, -0.7569, -0.7569,  ..., -0.5843, -0.6235, -0.6784],\n",
       "          ...,\n",
       "          [-0.6941, -0.7098, -0.7569,  ..., -0.5922, -0.5216, -0.5294],\n",
       "          [-0.7412, -0.6941, -0.7333,  ..., -0.5373, -0.5137, -0.5294],\n",
       "          [-0.7647, -0.7020, -0.7176,  ..., -0.4902, -0.4745, -0.5059]],\n",
       "\n",
       "         [[-0.5922, -0.6000, -0.6078,  ..., -0.2392, -0.5608, -0.2235],\n",
       "          [-0.6627, -0.6549, -0.6627,  ..., -0.4745, -0.6000, -0.5373],\n",
       "          [-0.6471, -0.6471, -0.6471,  ..., -0.4902, -0.5294, -0.5608],\n",
       "          ...,\n",
       "          [-0.6078, -0.6471, -0.7098,  ..., -0.5529, -0.4824, -0.4902],\n",
       "          [-0.6549, -0.6392, -0.6863,  ..., -0.5137, -0.4902, -0.5059],\n",
       "          [-0.6863, -0.6471, -0.6706,  ..., -0.4745, -0.4667, -0.4980]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x #comprobamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 4, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #comprobamos las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transforms)  #descarga el dataset de pytorch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset_cifar_test,  batch_size=batch_size_test, shuffle=True, num_workers=0,pin_memory=True)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos los datos preparados, ahora vamos a crear la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   =   0.001\n",
    "momentum   =   0.9\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        n_hidden1 = 128\n",
    "        n_hidden2 = 64\n",
    "        self.fc1 = nn.Linear(28*28,n_hidden1) #capa \"fuly connect\" entrada 28*28 (tama침o de la imagen) 50 neuronas\n",
    "        self.fc1_drop = nn.Dropout(0.2) #dropout (regularizacion) 20% \n",
    "        self.fc2 = nn.Linear(n_hidden1, n_hidden2) #capa fully connect 50 neuronas \n",
    "        self.fc2_drop = nn.Dropout(0.2) #dropout (regularizacion) 20%\n",
    "        self.fc3 = nn.Linear(n_hidden2, 10) #capa de salida numero de salida igual al de etiquetas\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) #cambia la forma del tensor, -1 para quitar la dimensiones anteriores y dejarlo todo en una vector de 256 elementos\n",
    "        x = F.relu(self.fc1(x)) #capa fully connect y luego activacion relu\n",
    "        x = self.fc1_drop(x) #dropout (regularizacion)\n",
    "        x = F.relu(self.fc2(x)) #capa fully connect y luego activacion relu\n",
    "        x = self.fc2_drop(x) #dropout (regularizacion)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1) #soft max (estimacion estadistica 0-1 de la probabildad de que sea de un etiqueta u otra)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion =  nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62006"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_dl, val_dl, epochs=20, device='cuda'):\n",
    "    '''\n",
    "    Runs training loop for classification problems. Returns Keras-style\n",
    "    per-epoch history of loss and accuracy over training and validation data.\n",
    "\n",
    "    Parametros\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Neural network model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Search space optimizer (e.g. Adam)\n",
    "    loss_fn :\n",
    "        Funcion de perdida (e.g. nn.CrossEntropyLoss())\n",
    "    train_dl : \n",
    "        Dataloader para los datos de entrenamiento.\n",
    "    val_dl :\n",
    "        Dataloader para los datos de validacion.\n",
    "    epochs : int\n",
    "        Numero de epocas\n",
    "    device : string\n",
    "        'cuda' para entrenamiento en gpu y 'cpu' para entrenamiento en cpu\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    Diccionario\n",
    "        Similar to Keras' fit(), the output dictionary contains per-epoch\n",
    "        history of training loss, training accuracy, validation loss, and\n",
    "        validation accuracy.\n",
    "    '''\n",
    "\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "\n",
    "    history = {} # Collects per-epoch loss and acc like Keras' fit().\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    start_time_sec = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # --- Entrenamiento en los datos de entrenamientos -----------------------------\n",
    "        model.train() #modelo en forma de entrenamiento    \n",
    "        train_loss         = 0.0 #perdida inicializada en cero\n",
    "        num_train_correct  = 0 #numero de elementos donde la prediccion coincide con la etiqueta \"true positive\"\n",
    "        num_train_examples = 0 #inicializa el numero de elementos sobre los que se ha realizado entrenamiento\n",
    "\n",
    "        for batch in train_dl:\n",
    "\n",
    "            optimizer.zero_grad() #pone los gradientes a cero\n",
    "\n",
    "            x    = batch[0].to(device) #datos al dispositivo (cpu o gpu)\n",
    "            y    = batch[1].to(device) #idem\n",
    "            yhat = model(x) # y predichos \n",
    "            loss = loss_fn(yhat, y) #calcula la perdida entre los \"y predichos\" y los \"y etiquetas\"\n",
    "\n",
    "            loss.backward() #retropropagaci칩n\n",
    "            optimizer.step() #se mueve el gradiente un paso (learning rate por el gradiente negativo)\n",
    "\n",
    "            train_loss         += loss.data.item() * x.size(0) \n",
    "            num_train_correct  += (torch.max(yhat, 1)[1] == y).sum().item() #compureba los elementos correctos\n",
    "            num_train_examples += x.shape[0]\n",
    "\n",
    "        train_acc   = num_train_correct / num_train_examples\n",
    "        train_loss  = train_loss / len(train_dl.dataset)\n",
    "\n",
    "\n",
    "        # --- Evaluacion en los datos de test -------------------------------------\n",
    "        model.eval() #capas de droput y normalizacion apagadas\n",
    "        val_loss       = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in val_dl:\n",
    "            with torch.no_grad(): #gradiente desactivados (menos memoria mas rapido)\n",
    "                x    = batch[0].to(device)\n",
    "                y    = batch[1].to(device)\n",
    "                yhat = model(x)\n",
    "                loss = loss_fn(yhat, y)\n",
    "\n",
    "                val_loss         += loss.data.item() * x.size(0)\n",
    "                num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "                num_val_examples += y.shape[0]\n",
    "\n",
    "        val_acc  = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(val_dl.dataset)\n",
    "\n",
    "\n",
    "        print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "              (epoch+1, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "    # END OF TRAINING LOOP\n",
    "\n",
    "\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=Net, opt=SGD(lr=0.010000), epochs=20, device=cuda\n",
      "\n",
      "Epoch   1/ 20, train loss:  1.63, train acc:  0.40, val loss:  1.40, val acc:  0.50\n",
      "Epoch   2/ 20, train loss:  1.35, train acc:  0.52, val loss:  1.30, val acc:  0.54\n",
      "Epoch   3/ 20, train loss:  1.26, train acc:  0.56, val loss:  1.27, val acc:  0.55\n",
      "Epoch   4/ 20, train loss:  1.19, train acc:  0.58, val loss:  1.25, val acc:  0.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\pytorch_mnist\\cifar.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/cifar.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train(model,optimizer,criterion, train_loader, test_loader, n_epochs, \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32md:\\Code\\pytorch_mnist\\cifar.ipynb Cell 37\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_dl, val_dl, epochs, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/cifar.ipynb#X51sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m num_train_correct  \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m#numero de elementos donde la prediccion coincide con la etiqueta \"true positive\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/cifar.ipynb#X51sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m num_train_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m#inicializa el numero de elementos sobre los que se ha realizado entrenamiento\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/cifar.ipynb#X51sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dl:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/cifar.ipynb#X51sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m#pone los gradientes a cero\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/cifar.ipynb#X51sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     x    \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device) \u001b[39m#datos al dispositivo (cpu o gpu)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(pic\u001b[39m.\u001b[39mgetbands()))\n\u001b[0;32m    169\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(model,optimizer,criterion, train_loader, test_loader, n_epochs, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'results/model.pth')\n",
    "torch.save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('results/model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(len(history['loss'])), history['loss'], 'bo', label='Training loss')\n",
    "plt.plot(range(len(history['val_loss'])), history['val_loss'], c=\"red\",label='Val loss')\n",
    "plt.title('Training and test loss')\n",
    "plt.legend()\n",
    "plt.ylim([0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(len(history['acc'])), history['acc'], 'bo', label='Training acc')\n",
    "plt.plot(range(len(history['val_acc'])), history['val_acc'], c=\"red\",label='Val acc')\n",
    "plt.title('Training and test acc')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fda304a6f307d97b29aaae674164dfd82bf6d6216593c67b562cfafd3100d5c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
