{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "Ejemplo de clasificador con Pytorch, comentado en español. \n",
    "\n",
    "El cuaderno de Jupyter se ha desarrollado con Visual Code y puedes encontrar el código en https://github.com/josegemez/pytorch_mnist \n",
    "\n",
    "Se ha desarrollado en python 3.10.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "Con las primeras lineas vamos a importar los paquetes que básicos de Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 128\n",
    "batch_size_test = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms=torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize((0.1307,), (0.3081,)) #media y desviacion estandar para el dataset de mnist en concreto\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mnist = torchvision.datasets.MNIST('/files/', train=True, download=True, transform=transforms)  #descarga el dataset de pytorch                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset_mnist)) #tamaño de los datos de entrenamiento 80%\n",
    "test_size = len(dataset_mnist) - train_size #tamaño test\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset_mnist, [train_size, test_size]) #realiza un train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,  batch_size=batch_size_train, shuffle=True,num_workers=5,pin_memory=True) #cargador de los datos en batch de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que \"pinta\" tiene cada elemento del cargador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader)) #asignamos el primer batch a las variables x e y. La variable X contrendra las imagenes e y contrendra las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x es un batch, por lo que la primera dimensión coincidira con el batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(x[0].view(28,28).numpy()); # transformamos el primer elemento del batch una matriz de numpy y mostramos con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x #comprobamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 7, 3, 0, 6, 3, 9, 3, 2, 2, 9, 4, 6, 7, 7, 3, 4, 2, 9, 3, 3, 1, 2,\n",
       "        5, 9, 4, 7, 6, 1, 3, 3, 9, 9, 8, 9, 8, 5, 8, 3, 1, 3, 8, 1, 3, 1, 6, 9,\n",
       "        5, 3, 7, 3, 2, 3, 7, 3, 9, 4, 8, 5, 7, 4, 2, 5, 3, 5, 9, 2, 4, 9, 7, 5,\n",
       "        8, 1, 5, 9, 2, 9, 7, 3, 6, 1, 3, 1, 8, 9, 5, 3, 8, 5, 8, 7, 6, 4, 7, 8,\n",
       "        5, 5, 1, 9, 3, 6, 0, 3, 6, 7, 4, 0, 1, 7, 4, 7, 7, 8, 7, 4, 4, 3, 4, 6,\n",
       "        6, 0, 1, 1, 6, 1, 2, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #comprobamos las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size_test, shuffle=True, num_workers=5,pin_memory=True)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos los datos preparados, ahora vamos a crear la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   =   0.01 \n",
    "momentum   =   0.5\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        n_hidden1 = 128\n",
    "        n_hidden2 = 64\n",
    "        self.fc1 = nn.Linear(28*28,n_hidden1) #capa \"fuly connect\" entrada 28*28 (tamaño de la imagen) 50 neuronas\n",
    "        self.fc1_drop = nn.Dropout(0.2) #dropout (regularizacion) 20% \n",
    "        self.fc2 = nn.Linear(n_hidden1, n_hidden2) #capa fully connect 50 neuronas \n",
    "        self.fc2_drop = nn.Dropout(0.2) #dropout (regularizacion) 20%\n",
    "        self.fc3 = nn.Linear(n_hidden2, 10) #capa de salida numero de salida igual al de etiquetas\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) #cambia la forma del tensor, -1 para quitar la dimensiones anteriores y dejarlo todo en una vector de 256 elementos\n",
    "        x = F.relu(self.fc1(x)) #capa fully connect y luego activacion relu\n",
    "        x = self.fc1_drop(x) #dropout (regularizacion)\n",
    "        x = F.relu(self.fc2(x)) #capa fully connect y luego activacion relu\n",
    "        x = self.fc2_drop(x) #dropout (regularizacion)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1) #soft max (estimacion estadistica 0-1 de la probabildad de que sea de un etiqueta u otra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "((1*5*5)+1)*32 = 352\n",
    "704 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5) ## convolucion 2d  Width_out = (Width_in - Width_filter + 2 * padding) / stride +1 \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(2048, 1024) #input 320 caracteristicas\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) #convolucion1 #max polling # activacion relu 28x28\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) #convolucion1 #droping #max polling # activacion relu\n",
    "        x = x.view(-1, 2048) # 320 caracteristicas de entrada \n",
    "        x = F.relu(self.fc1(x)) # capa fully connectect con las 320 entradas y \n",
    "        x = F.dropout(x, training=self.training) #dropout cuando esta en entrenamiento \n",
    "        x = self.fc2(x) #segunda capa fully connect \n",
    "        return F.log_softmax(x, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 24, 24]             832\n",
      "            Conv2d-2             [-1, 64, 8, 8]          51,264\n",
      "         Dropout2d-3             [-1, 64, 8, 8]               0\n",
      "            Linear-4                 [-1, 1024]       2,098,176\n",
      "            Linear-5                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 2,160,522\n",
      "Trainable params: 2,160,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.21\n",
      "Params size (MB): 8.24\n",
      "Estimated Total Size (MB): 8.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion =  nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1144714"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_dl, val_dl, epochs=20, device='cuda'):\n",
    "    '''\n",
    "    Runs training loop for classification problems. Returns Keras-style\n",
    "    per-epoch history of loss and accuracy over training and validation data.\n",
    "\n",
    "    Parametros\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Neural network model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Search space optimizer (e.g. Adam)\n",
    "    loss_fn :\n",
    "        Funcion de perdida (e.g. nn.CrossEntropyLoss())\n",
    "    train_dl : \n",
    "        Dataloader para los datos de entrenamiento.\n",
    "    val_dl :\n",
    "        Dataloader para los datos de validacion.\n",
    "    epochs : int\n",
    "        Numero de epocas\n",
    "    device : string\n",
    "        'cuda' para entrenamiento en gpu y 'cpu' para entrenamiento en cpu\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    Diccionario\n",
    "        Similar to Keras' fit(), the output dictionary contains per-epoch\n",
    "        history of training loss, training accuracy, validation loss, and\n",
    "        validation accuracy.\n",
    "    '''\n",
    "\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "\n",
    "    history = {} # Collects per-epoch loss and acc like Keras' fit().\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    start_time_sec = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # --- Entrenamiento en los datos de entrenamientos -----------------------------\n",
    "        model.train() #modelo en forma de entrenamiento    \n",
    "        train_loss         = 0.0 #perdida inicializada en cero\n",
    "        num_train_correct  = 0 #numero de elementos donde la prediccion coincide con la etiqueta \"true positive\"\n",
    "        num_train_examples = 0 #inicializa el numero de elementos sobre los que se ha realizado entrenamiento\n",
    "\n",
    "        for batch in train_dl:\n",
    "\n",
    "            optimizer.zero_grad() #pone los gradientes a cero\n",
    "\n",
    "            x    = batch[0].to(device) #datos al dispositivo (cpu o gpu)\n",
    "            y    = batch[1].to(device) #idem\n",
    "            yhat = model(x) # y predichos \n",
    "            loss = loss_fn(yhat, y) #calcula la perdida entre los \"y predichos\" y los \"y etiquetas\"\n",
    "\n",
    "            loss.backward() #retropropagación\n",
    "            optimizer.step() #se mueve el gradiente un paso (learning rate por el gradiente negativo)\n",
    "\n",
    "            train_loss         += loss.data.item() * x.size(0) \n",
    "            num_train_correct  += (torch.max(yhat, 1)[1] == y).sum().item() #compureba los elementos correctos\n",
    "            num_train_examples += x.shape[0]\n",
    "\n",
    "        train_acc   = num_train_correct / num_train_examples\n",
    "        train_loss  = train_loss / len(train_dl.dataset)\n",
    "\n",
    "\n",
    "        # --- Evaluacion en los datos de test -------------------------------------\n",
    "        model.eval()\n",
    "        val_loss       = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in val_dl:\n",
    "            with torch.no_grad():\n",
    "                x    = batch[0].to(device)\n",
    "                y    = batch[1].to(device)\n",
    "                yhat = model(x)\n",
    "                loss = loss_fn(yhat, y)\n",
    "\n",
    "                val_loss         += loss.data.item() * x.size(0)\n",
    "                num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "                num_val_examples += y.shape[0]\n",
    "\n",
    "        val_acc  = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(val_dl.dataset)\n",
    "\n",
    "\n",
    "        print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "              (epoch+1, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "    # END OF TRAINING LOOP\n",
    "\n",
    "\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=Net, opt=SGD(lr=0.010000), epochs=20, device=cuda\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (64) to match target batch_size (128).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\pytorch_mnist\\mnist.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/mnist.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train(model,optimizer,criterion, train_loader, test_loader, n_epochs, \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32md:\\Code\\pytorch_mnist\\mnist.ipynb Cell 39\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_dl, val_dl, epochs, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/mnist.ipynb#X50sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m y    \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device) \u001b[39m#idem\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/mnist.ipynb#X50sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m yhat \u001b[39m=\u001b[39m model(x) \u001b[39m# y predichos \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/mnist.ipynb#X50sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(yhat, y) \u001b[39m#calcula la perdida entre los \"y predichos\" y los \"y etiquetas\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/mnist.ipynb#X50sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m#retropropagación\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/pytorch_mnist/mnist.ipynb#X50sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m#se mueve el gradiente un paso (learning rate por el gradiente negativo)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (128)."
     ]
    }
   ],
   "source": [
    "history = train(model,optimizer,criterion, train_loader, test_loader, n_epochs, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'results/model.pth')\n",
    "torch.save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('results/model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(len(history['loss'])), history['loss'], 'bo', label='Training loss')\n",
    "plt.plot(range(len(history['val_loss'])), history['val_loss'], c=\"red\",label='Val loss')\n",
    "plt.title('Training and test loss')\n",
    "plt.legend()\n",
    "plt.ylim([0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(len(history['acc'])), history['acc'], 'bo', label='Training acc')\n",
    "plt.plot(range(len(history['val_acc'])), history['val_acc'], c=\"red\",label='Val acc')\n",
    "plt.title('Training and test acc')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fda304a6f307d97b29aaae674164dfd82bf6d6216593c67b562cfafd3100d5c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
